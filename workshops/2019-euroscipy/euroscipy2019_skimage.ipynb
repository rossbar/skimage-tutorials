{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1eecbf9",
   "metadata": {},
   "source": [
    "# EuroSciPy 2019 - 3D image processing with scikit-image\n",
    "\n",
    "* Support material for the tutorial _3D image processing with scikit-image_.\n",
    "\n",
    "This tutorial will introduce how to analyze three dimensional stacked and volumetric images in Python, mainly using scikit-image. Here we will learn how to:\n",
    " * pre-process data using filtering, binarization and segmentation techniques.\n",
    " * inspect, count and measure attributes of objects and regions of interest in the data.\n",
    " * visualize large 3D data.\n",
    "\n",
    "For more info:\n",
    "  * [[EuroSciPy (all editions)]](https://www.euroscipy.org/)\n",
    "  * [[EuroSciPy 2019]](https://www.euroscipy.org/2019/)\n",
    "  * [[scikit-image]](https://scikit-image.org/)\n",
    "  * [[scikit-image tutorials]](https://github.com/scikit-image/skimage-tutorials)\n",
    "\n",
    "Please refer to the scikit-image tutorials when using this material.\n",
    "\n",
    "## What is scikit-image?\n",
    "\n",
    "scikit-image is a collection of image processing algorithms which aims to integrate well with for the SciPy ecosystem.\n",
    "\n",
    "It is well documented, and provides well-tested code to quickly build sophisticated image processing pipelines.\n",
    "\n",
    "\n",
    "## Checking the system\n",
    "\n",
    "First, we'll check if your system have the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run check_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c90ae5",
   "metadata": {},
   "source": [
    "## Importing the base Scientific Python ecossystem\n",
    "\n",
    "Let's start importing the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03043bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb668bf7",
   "metadata": {},
   "source": [
    "Then, let's set a nice, `monospace` font for matplotlib's figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'monospace'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127d03d",
   "metadata": {},
   "source": [
    "## Introduction to three-dimensional image processing\n",
    "\n",
    "In scikit-image, images are represented as `numpy` arrays.\n",
    "\n",
    "A grayscale image is a 2D matrix of pixel intensities of shape `(row, column)`. They are also called single-channel images. Multi-channel data has an extra dimension, `channel`, in the final position. `channel` contains color information. \n",
    "\n",
    "We can construct a 3D volume as a series of 2D `planes`, giving 3D images the shape `(plane, row, column)`.\n",
    "\n",
    "Summarizing:\n",
    "\n",
    "|Image type|Coordinates|\n",
    "|:---|:---|\n",
    "|2D grayscale|(row, column)|\n",
    "|2D multichannel|(row, column, channel)|\n",
    "|3D grayscale|(plane, row, column)|\n",
    "|3D multichannel|(plane, row, column, channel)|\n",
    "\n",
    "Some 3D images are constructed with equal resolution in each dimension. An example would be a computer generated rendering of a sphere with dimensions `(30, 30, 30)`: 30 planes, 30 rows and 30 columns.\n",
    "\n",
    "However, most experimental data captures one dimension at a lower resolution than the other two. For example, photographing thin slices to approximate a 3D structure as a stack of 2D images. We will work with one example of such data in this tutorial.\n",
    "\n",
    "\n",
    "## [skimage.io](https://scikit-image.org/docs/stable/api/skimage.io.html) - utilities to read and write images in various formats<a id='io'></a>\n",
    "\n",
    "This module helps us on reading images and saving the results. There are multiple plugins available, which support multiple formats. The most commonly used functions include:\n",
    "\n",
    "* `io.imread`: read an image to a numpy array.\n",
    "* `io.imsave`: write an image to disk.\n",
    "* `io.imread_collection`: read multiple images which match a common pattern.\n",
    "\n",
    "Data can be loaded with `io.imread`, as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io  # skimage's I/O submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = io.imread('../../images/cells.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b68e88",
   "metadata": {},
   "source": [
    "First let's check its shape, data type and range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfba61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('* \"cells\" shape: {}'.format(cells.shape))\n",
    "print('* \"cells\" type: {}'.format(cells.dtype))\n",
    "print('* \"cells\" range: {}, {}'.format(cells.min(), cells.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28880b5",
   "metadata": {},
   "source": [
    "We see that `cells` has 60 planes, each with 256 rows and 256 columns. Let's try visualizing the image with `skimage.io.imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be47d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    io.imshow(cells, cmap='gray')\n",
    "except TypeError as error:\n",
    "    print(str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5c650",
   "metadata": {},
   "source": [
    "`skimage.io.imshow` can only display grayscale and RGB(A) 2D images. We can use `skimage.io.imshow` to visualize 2D planes. Let's use some helping functions for checking 3D data, then.\n",
    "\n",
    "All supplementary functions we will use during this tutorial are stored within `supplementary_code.py`. First, we import this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supplementary_code as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb23ba4",
   "metadata": {},
   "source": [
    "By fixing one axis, we can observe three different views of the image. Let's use the helper function `show_plane` to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a1a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (win_left, win_center, win_right) = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "sc.show_plane(win_left, cells[32], title='Plane = 32')\n",
    "sc.show_plane(win_center, cells[:, 128, :], title='Row = 128')\n",
    "sc.show_plane(win_right, cells[:, :, 128], title='Column = 128')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772e9fd",
   "metadata": {},
   "source": [
    "Three-dimensional images can be viewed as a series of two-dimensional ones. The `slice_explorer` helper presents a slider to check the 2D planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.slice_explorer(cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28886822",
   "metadata": {},
   "source": [
    "The `display` helper function, on the other hand, displays 30 planes of the provided image. By default, every other plane is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff82dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.display(cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956e819",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(3 min, shall we? 🙄)</font>__ there is another dataset within the folder `image`, called `bead_pack.tif`.\n",
    "\n",
    "Now, using what we saw so far, there's some tasks for you:\n",
    "  * Read this data and check its shape, data type, minimum and maximum values.\n",
    "  * Check the slices using the function `slice_explorer`.\n",
    "  * Display each six slices using the function `display` (you will use the variable `step` for that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!\n",
    "beadpack = io.imread('data/bead_pack.tif')\n",
    "\n",
    "print('* \"beadpack\" shape: {}'.format(beadpack.shape))\n",
    "print('* \"beadpack\" type: {}'.format(beadpack.dtype))\n",
    "print('* \"beadpack\" range: {}, {}'.format(beadpack.min(),\n",
    "                                          beadpack.max()))\n",
    "\n",
    "sc.slice_explorer(beadpack)\n",
    "sc.display(beadpack, step=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5930f",
   "metadata": {},
   "source": [
    "## [skimage.exposure](https://scikit-image.org/docs/stable/api/skimage.exposure.html) - evaluating or changing the exposure of an image<a id='exposure'></a>\n",
    "\n",
    "This module contains a number of functions for adjusting image contrast. We will use some of them:\n",
    "\n",
    "* `exposure.adjust_gamma`: gamma correction.\n",
    "* `exposure.equalize_hist`: histogram equalization.\n",
    "\n",
    "[Gamma correction](https://en.wikipedia.org/wiki/Gamma_correction), also known as Power Law Transform, brightens or darkens an image. The function $O = I^\\gamma$ is applied to each pixel in the image. A `gamma < 1` will brighten an image, while a `gamma > 1` will darken an image.\n",
    "\n",
    "One of the most common tools to evaluate exposure is the *histogram*, which plots the number of points which have a certain value against the values in order from lowest (dark) to highest (light)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure  # skimage's exposure module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df99c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_val_low = 0.5\n",
    "cells_gamma_low = exposure.adjust_gamma(cells, gamma=gamma_val_low)\n",
    "\n",
    "gamma_val_high = 1.5\n",
    "cells_gamma_high = exposure.adjust_gamma(cells, gamma=gamma_val_high)\n",
    "\n",
    "_, ((win_top_left, win_top_center, win_top_right),\n",
    "    (win_bottom_left, win_bottom_center, win_bottom_right)) = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
    "\n",
    "# Original and its histogram.\n",
    "sc.show_plane(win_top_left, cells[32], title='Original')\n",
    "sc.plot_hist(win_bottom_left, cells)\n",
    "\n",
    "# Gamma = 0.5 and its histogram.\n",
    "sc.show_plane(win_top_center, cells_gamma_low[32], title='Gamma = {}'.format(gamma_val_low))\n",
    "sc.plot_hist(win_bottom_center, cells_gamma_low)\n",
    "\n",
    "# Gamma = 1.5 and its histogram.\n",
    "sc.show_plane(win_top_right, cells_gamma_high[32], title='Gamma = {}'.format(gamma_val_high))\n",
    "sc.plot_hist(win_bottom_right, cells_gamma_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994a430",
   "metadata": {},
   "source": [
    "[Histogram equalization](https://en.wikipedia.org/wiki/Histogram_equalization) improves contrast in an image by redistributing pixel intensities. The most common pixel intensities are spread out, allowing areas of lower local contrast to gain a higher contrast. This may enhance background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313424cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_equalized = exposure.equalize_hist(cells)\n",
    "\n",
    "sc.slice_explorer(cells_equalized)\n",
    "\n",
    "_, ((win_top_left, win_top_right),\n",
    "    (win_bottom_left, win_bottom_right)) = plt.subplots(nrows=2, ncols=2, figsize=(16, 8))\n",
    "\n",
    "sc.plot_hist(win_top_left, cells, title='Original')\n",
    "sc.plot_hist(win_top_right, cells_equalized, title='Histogram equalization')\n",
    "\n",
    "cdf, bins = exposure.cumulative_distribution(cells.ravel())\n",
    "win_bottom_left.plot(bins, cdf, 'r')\n",
    "win_bottom_left.set_title('Original CDF')\n",
    "\n",
    "cdf, bins = exposure.cumulative_distribution(cells_equalized.ravel())\n",
    "win_bottom_right.plot(bins, cdf, 'r')\n",
    "win_bottom_right.set_title('Histogram equalization CDF');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431327c",
   "metadata": {},
   "source": [
    "Most experimental images are affected by salt and pepper noise. A few bright artifacts can decrease the relative intensity of the pixels of interest. A simple way to improve contrast is to clip the pixel values on the lowest and highest extremes. Clipping the darkest and brightest 0.5% of pixels will increase the overall contrast of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = np.percentile(cells, q=(0.5, 99.5))\n",
    "\n",
    "cells_clipped = exposure.rescale_intensity(\n",
    "    cells,\n",
    "    in_range=(vmin, vmax), \n",
    "    out_range=np.float32\n",
    ")\n",
    "\n",
    "sc.slice_explorer(cells_clipped);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880b17f",
   "metadata": {},
   "source": [
    "We'll call our dataset `cells_rescaled` from now on. In this cell, you can choose any of the previous results to continue working with.\n",
    "\n",
    "In the next steps, we'll use the `cells_clipped` version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a303d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_rescaled = cells_clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2061fb",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(7-ish min? 🙄)</font>__ now, using our variable `beadpack`, let's repeat the process, ok?\n",
    "\n",
    "Now, using what we saw so far, there's some tasks for you:\n",
    "  * Obtain a nice `gamma_val` to adjust the gamma of `beadpack`.\n",
    "  * Equalize `beadpack`'s histogram using `equalize_hist` and CLAHE (given by `equalize_adapthist`).\n",
    "  * Increase `beadpack`'s contrast by clipping the darkest/brightest pixels there. Try different percentages.\n",
    "  * Choose the data you think is best, and call it `beadpack_rescaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e11423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part #1 of your solution goes here!\n",
    "gamma_val = 0.7\n",
    "beadpack_gamma = exposure.adjust_gamma(beadpack, gamma=gamma_val)\n",
    "\n",
    "_, ((win_top_left, win_top_right),\n",
    "    (win_bottom_left, win_bottom_right)) = plt.subplots(nrows=2, ncols=2, figsize=(16, 8))\n",
    "\n",
    "# Original and its histogram.\n",
    "sc.show_plane(win_top_left, beadpack[32], title='Original')\n",
    "sc.plot_hist(win_bottom_left, beadpack)\n",
    "\n",
    "# Gamma-adjusted and its histogram.\n",
    "sc.show_plane(win_top_right, beadpack_gamma[32], title='Gamma = {}'.format(gamma_val))\n",
    "sc.plot_hist(win_bottom_right, beadpack_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0160f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part #2 of your solution goes here!\n",
    "\n",
    "# let's convert beadpack to float; it'll help us on the future.\n",
    "from skimage import util\n",
    "beadpack = util.img_as_float(beadpack)\n",
    "\n",
    "# First, let's create a version using histogram equalization. \n",
    "beadpack_equalized = exposure.equalize_hist(beadpack)\n",
    "sc.slice_explorer(beadpack_equalized)\n",
    "\n",
    "# Now, a version using CLAHE. \n",
    "beadpack_clahe = np.empty_like(beadpack)\n",
    "\n",
    "for plane, image in enumerate(beadpack):\n",
    "    beadpack_clahe[plane] = exposure.equalize_adapthist(image)\n",
    "sc.slice_explorer(beadpack_clahe)\n",
    "\n",
    "# Let's check the results.\n",
    "_, ((win_top_left, win_top_center, win_top_right),\n",
    "    (win_bottom_left, win_bottom_center, win_bottom_right)) = plt.subplots(nrows=2, ncols=3, figsize=(16, 8))\n",
    "\n",
    "sc.plot_hist(win_top_left, beadpack, title='Original')\n",
    "sc.plot_hist(win_top_center, beadpack_equalized, title='Histogram equalization')\n",
    "sc.plot_hist(win_top_right, beadpack_clahe, title='CLAHE')\n",
    "\n",
    "cdf, bins = exposure.cumulative_distribution(beadpack.ravel())\n",
    "win_bottom_left.plot(bins, cdf, 'r')\n",
    "win_bottom_left.set_title('Original CDF')\n",
    "\n",
    "cdf, bins = exposure.cumulative_distribution(beadpack_equalized.ravel())\n",
    "win_bottom_center.plot(bins, cdf, 'r')\n",
    "win_bottom_center.set_title('Histogram equalization CDF');\n",
    "\n",
    "cdf, bins = exposure.cumulative_distribution(beadpack_clahe.ravel())\n",
    "win_bottom_right.plot(bins, cdf, 'r')\n",
    "win_bottom_right.set_title('CLAHE CDF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aaad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part #3 of your solution goes here!\n",
    "vmin, vmax = np.percentile(data, q=)\n",
    "\n",
    "beadpack_clipped = exposure.rescale_intensity(\n",
    "    beadpack,\n",
    "    in_range=(vmin, vmax), \n",
    "    out_range=np.float32\n",
    ")\n",
    "\n",
    "sc.slice_explorer(beadpack_clipped);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050cf06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, choose your destiny!\n",
    "beadpack_rescaled = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7cad8",
   "metadata": {},
   "source": [
    "## Edge detection\n",
    "\n",
    "[Edge detection](https://en.wikipedia.org/wiki/Edge_detection) highlights regions in the image where a sharp change in contrast occurs. The intensity of an edge corresponds to the steepness of the transition from one intensity to another. A gradual shift from bright to dark intensity results in a dim edge. An abrupt shift results in a bright edge.\n",
    "\n",
    "The [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator) is an edge detection algorithm which approximates the gradient of the image intensity, and is fast to compute.\n",
    "\n",
    "\n",
    "## [skimage.filters](https://scikit-image.org/docs/stable/api/skimage.filters.html) - apply filters to an image<a id='filters'></a>\n",
    "\n",
    "Filtering applies whole-image modifications such as sharpening or blurring. In addition to edge detection, `skimage.filters` provides functions for filtering and thresholding images.\n",
    "\n",
    "Notable functions include (links to relevant gallery examples):\n",
    "\n",
    "* [Thresholding](https://scikit-image.org/docs/stable/auto_examples/applications/plot_thresholding.html):\n",
    "  * `filters.threshold_*` (multiple different functions with this prefix)\n",
    "  * `filters.try_all_threshold` to compare various methods\n",
    "* [Edge finding/enhancement](https://scikit-image.org/docs/stable/auto_examples/edges/plot_edge_filter.html):\n",
    "  * `filters.sobel` - not adapted for 3D images. It can be applied planewise to approximate a 3D result.\n",
    "  * `filters.prewitt`\n",
    "  * `filters.scharr`\n",
    "  * `filters.roberts`\n",
    "  * `filters.laplace`\n",
    "  * `filters.hessian`\n",
    "* [Ridge filters](https://scikit-image.org/docs/stable/auto_examples/edges/plot_ridge_filter.html):\n",
    "  * `filters.meijering`\n",
    "  * `filters.sato`\n",
    "  * `filters.frangi`\n",
    "* Inverse filtering (see also [skimage.restoration](#restoration)):\n",
    "  * `filters.weiner`\n",
    "  * `filters.inverse`\n",
    "* [Directional](https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_gabor.html): `filters.gabor`\n",
    "* Blurring/denoising\n",
    "  * `filters.gaussian`\n",
    "  * `filters.median`\n",
    "* [Sharpening](https://scikit-image.org/docs/stable/auto_examples/filters/plot_unsharp_mask.html): `filters.unsharp_mask`\n",
    "* Define your own filter: `LPIFilter2D`\n",
    "  \n",
    "The sub-submodule `skimage.filters.rank` contains rank filters. These filters are nonlinear and operate on the local histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4146bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters  # skimage's filtering module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fe461",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_sobel = np.empty_like(cells_rescaled)\n",
    "\n",
    "for plane, image in enumerate(cells_rescaled):\n",
    "    cells_sobel[plane] = filters.sobel(image)\n",
    "    \n",
    "sc.slice_explorer(cells_sobel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321683c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ((win_top_left, win_top_right),\n",
    "    (win_bottom_left, win_bottom_right)) = plt.subplots(nrows=2, ncols=2, figsize=(16, 4))\n",
    "\n",
    "sc.show_plane(win_top_left, cells_sobel[:, 128, :], title='3D sobel, row = 128')\n",
    "\n",
    "cells_sobel_row = filters.sobel(cells_rescaled[:, 128, :])\n",
    "sc.show_plane(win_top_right, cells_sobel_row, title='2D sobel, row=128')\n",
    "\n",
    "sc.show_plane(win_bottom_left, cells_sobel[:, :, 128], title='3D sobel, column = 128')\n",
    "\n",
    "cells_sobel_col = filters.sobel(cells_rescaled[:, :, 128])\n",
    "sc.show_plane(win_bottom_right, cells_sobel_col, title='2D sobel, column=128')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b8c38",
   "metadata": {},
   "source": [
    "## [skimage.transform](https://scikit-image.org/docs/stable/api/skimage.transform.html) - transforms & warping<a id='transform'></a>\n",
    "\n",
    "This submodule has multiple features which fall under the umbrella of transformations.\n",
    "\n",
    "Forward (`radon`) and inverse (`iradon`) radon transforms, as well as some variants (`iradon_sart`) and the finite versions of these transforms (`frt2` and `ifrt2`).  These are used for [reconstructing medical computed tomography (CT) images](https://scikit-image.org/docs/stable/auto_examples/transform/plot_radon_transform.html).\n",
    "\n",
    "Hough transforms for identifying lines, circles, and ellipses.\n",
    "\n",
    "Changing image size, shape, or resolution with `resize`, `rescale`, or `downscale_local_mean`.\n",
    "\n",
    "`warp`, and `warp_coordinates` which take an image or set of coordinates and translate them through one of the defined `*Transforms` in this submodule.  `estimate_transform` may be assist in estimating the parameters.\n",
    "\n",
    "[Numerous gallery examples are available](https://scikit-image.org/docs/stable/auto_examples/index.html#geometrical-transformations-and-registration) illustrating these functions.  [The panorama tutorial also includes warping](./solutions/adv3_panorama-stitching-solution.ipynb) via `SimilarityTransform` with parameter estimation via `measure.ransac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform  # skimage's transform submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf387fce",
   "metadata": {},
   "source": [
    "We created the illustration below to illustrate the downsampling operation. The red dots show the pixels within each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6bf61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure we all see the same thing, let's set a seed\n",
    "np.random.seed(0)\n",
    "\n",
    "image = np.random.random((8, 8))\n",
    "image_rescaled = transform.downscale_local_mean(image, (4, 4))\n",
    "\n",
    "_, (win_left, win_right) = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "win_left.imshow(image, cmap='gray')\n",
    "win_left.set_xticks([])\n",
    "win_left.set_yticks([])\n",
    "centers = np.indices(image.shape).reshape(2, -1).T\n",
    "win_left.plot(centers[:, 0], centers[:, 1], '.r')\n",
    "win_left.set_title('Original: {}'.format(image.shape))\n",
    "\n",
    "win_right.imshow(image_rescaled, cmap='gray')\n",
    "win_right.set_xticks([])\n",
    "win_right.set_yticks([])\n",
    "centers = np.indices(image_rescaled.shape).reshape(2, -1).T\n",
    "win_right.plot(centers[:, 0], centers[:, 1], '.r');\n",
    "win_right.set_title('Downsampled: {}'.format(image_rescaled.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e1e10",
   "metadata": {},
   "source": [
    "The distance between pixels in each dimension, called `spacing`, is encoded in a tuple and is accepted as a parameter by some `skimage` functions and can be used to adjust contributions to filters.\n",
    "\n",
    "The distance between pixels was reported by the microscope used to image the cells. This `spacing` information will be used to adjust contributions to filters and helps decide when to apply operations planewise. We've chosen to downsample each slice by a factor of 4 in the `row` and `column` dimensions to make the data smaller, thus reducing computational time. We also normalize it to `1.0` in the `row` and `column` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The microscope reports the following spacing:\n",
    "original_spacing = np.array([0.2900000, 0.0650000, 0.0650000])\n",
    "print('* Microscope original spacing: {}'.format(original_spacing))\n",
    "\n",
    "# We downsampled each slice 4x to make the data smaller\n",
    "rescaled_spacing = original_spacing * [1, 4, 4]\n",
    "print('* Microscope after rescaling images: {}'.format(rescaled_spacing))\n",
    "\n",
    "# Normalize the spacing so that pixels are a distance of 1 apart\n",
    "spacing = rescaled_spacing / rescaled_spacing[2]\n",
    "print('* Microscope normalized spacing: {}'.format(spacing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f7b70",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(3-ish min? 🙄)</font>__ now, using our variable `beadpack_rescaled`, let's check its edges.\n",
    "\n",
    "Your tasks right now are:\n",
    "  * Use the Sobel edge filter to obtain the edges of `beadpack_rescaled`.\n",
    "  * Explore the edges at each depth.\n",
    "  * Check 2D and 3D Sobel filters when row and column are equal to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10045315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!\n",
    "beadpack_sobel = np.empty_like()\n",
    "\n",
    "for plane, image in enumerate():\n",
    "    beadpack_sobel[plane] = filters.sobel(image)\n",
    "\n",
    "sc.slice_explorer(beadpack_sobel)\n",
    "\n",
    "_, ((win_top_left, win_top_right),\n",
    "    (win_bottom_left, win_bottom_right)) = plt.subplots(nrows=2, ncols=2, figsize=(16, 14))\n",
    "\n",
    "sc.show_plane(win_top_left, , title='3D sobel, row=100')\n",
    "\n",
    "beadpack_sobel_row = filters.sobel()\n",
    "sc.show_plane(win_top_right, , title='2D sobel, row=100')\n",
    "\n",
    "sc.show_plane(win_bottom_left, , title='3D sobel, column=100')\n",
    "\n",
    "beadpack_sobel_col = filters.sobel()\n",
    "sc.show_plane(win_bottom_right, , title='2D sobel, column=100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e6147",
   "metadata": {},
   "source": [
    "## Filters\n",
    "\n",
    "[Gaussian filter](https://en.wikipedia.org/wiki/Gaussian_filter) applies a Gaussian function to an image, creating a smoothing effect. `skimage.filters.gaussian` takes as input `sigma` which can be a scalar or a sequence of scalar. This `sigma` determines the standard deviation of the Gaussian along each axis. When the resolution in the `plane` dimension is much worse than the `row` and `column` dimensions, dividing `base_sigma` by the image `spacing` will balance the contribution to the filter along each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd00df",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sigma = 2.0\n",
    "sigma = base_sigma / spacing\n",
    "\n",
    "cells_gaussian = filters.gaussian(cells_rescaled, multichannel=False, sigma=sigma)\n",
    "\n",
    "sc.slice_explorer(cells_gaussian);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f512b1f",
   "metadata": {},
   "source": [
    "[Median filter](https://en.wikipedia.org/wiki/Median_filter) is a noise removal filter. It is particularly effective against salt and pepper noise. An additional feature of the median filter is its ability to preserve edges. This is helpful in segmentation because the original shape of regions of interest will be preserved.\n",
    "\n",
    "`skimage.filters.median` does not support three-dimensional images and needs to be applied planewise.\n",
    "\n",
    "## [skimage.util](https://scikit-image.org/docs/stable/api/skimage.util.html) - utility functions<a id='util'></a>\n",
    "\n",
    "These are generally useful functions which have no definite other place in the package.\n",
    "\n",
    "* `util.img_as_*` are convenience functions for datatype conversion.\n",
    "\n",
    "* `util.invert` is a convenient way to invert any image, accounting for its datatype.\n",
    "\n",
    "* `util.random_noise` is a comprehensive function to apply any amount of many different types of noise to images.  The seed may be set, resulting in pseudo-random noise for testing.\n",
    "\n",
    "* `util.view_as_*` allows for overlapping views into the same memory array, which is useful for elegant local computations with minimal memory impact.\n",
    "\n",
    "* `util.apply_parallel` uses Dask to apply a function across subsections of an image.  This can result in dramatic performance or memory improvements, but depending on the algorithm edge effects or lack of knowledge of the remainder of the image may result in unexpected results.\n",
    "\n",
    "* `util.pad` and `util.crop` pads or crops the edges of images.  `util.pad` is now a direct wrapper for `numpy.pad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af405cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import util  # skimage's util submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_rescaled_ubyte = util.img_as_ubyte(cells_rescaled)\n",
    "\n",
    "cells_median = np.empty_like(cells_rescaled_ubyte)\n",
    "\n",
    "for plane, image in enumerate(cells_rescaled_ubyte):\n",
    "    cells_median[plane] = filters.median(image)\n",
    "    \n",
    "cells_median = util.img_as_float(cells_median)\n",
    "    \n",
    "sc.slice_explorer(cells_median);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469cd7f",
   "metadata": {},
   "source": [
    "## [skimage.restoration](https://scikit-image.org/docs/stable/api/skimage.restoration.html) - restoration of an image<a id='restoration'></a>\n",
    "\n",
    "This submodule includes routines to restore images.  Currently these routines fall into four major categories.  Links lead to topical gallery examples.\n",
    "\n",
    "* `restoration.denoise_*` - [Reducing noise](https://scikit-image.org/docs/stable/auto_examples/filters/plot_denoise.html).\n",
    "* [Deconvolution](https://scikit-image.org/docs/stable/auto_examples/filters/plot_deconvolution.html), or reversing a convolutional effect which applies to the entire image. This can be done in an [unsupervised](https://scikit-image.org/docs/stable/auto_examples/filters/plot_restoration.html) way.\n",
    "  * `restoration.weiner`\n",
    "  * `restoration.unsupervised_weiner`\n",
    "  * `restoration.richardson_lucy`\n",
    "* `restoration.inpaint_biharmonic` - [Inpainting](https://scikit-image.org/docs/stable/auto_examples/filters/plot_inpaint.html), or filling in missing areas of an image.\n",
    "* `restoration.unwrap_phase` - [Phase unwrapping](https://scikit-image.org/docs/stable/auto_examples/filters/plot_phase_unwrap.html).\n",
    "\n",
    "A [bilateral filter](https://en.wikipedia.org/wiki/Bilateral_filter) is another edge-preserving, denoising filter. Each pixel is assigned a weighted average based on neighboring pixels. The weight is determined by spatial and radiometric similarity (e.g., distance between two colors).\n",
    "\n",
    "`skimage.restoration.denoise_bilateral` requires a `multichannel` parameter. This determines whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension. While the function does not yet support 3D data, the `multichannel` parameter will help distinguish multichannel 2D data from grayscale 3D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daccb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import restoration  # skimage's restoration submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_bilateral = np.empty_like(cells_rescaled)\n",
    "\n",
    "for plane, image in enumerate(cells_rescaled):\n",
    "    cells_bilateral[plane] = restoration.denoise_bilateral(\n",
    "        image, \n",
    "        multichannel=False\n",
    "    )\n",
    "\n",
    "sc.slice_explorer(cells_bilateral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b15288",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ((win_top_left, win_top_right),\n",
    "    (win_bottom_left, win_bottom_right)) = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "\n",
    "sc.show_plane(win_top_left, cells_rescaled[32], title='Original')\n",
    "sc.show_plane(win_top_right, cells_gaussian[32], title='Gaussian')\n",
    "sc.show_plane(win_bottom_left, cells_bilateral[32], title='Bilateral')\n",
    "sc.show_plane(win_bottom_right, cells_median[32], title='Median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_denoised = cells_median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832fbb3a",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(5-ish min? 🙄)</font>__ let's filter `beadpack_rescaled` now.\n",
    "\n",
    "Your tasks are:\n",
    "  * Use Gaussian, median and bilateral filters on `beadpack_rescaled`.\n",
    "  * Check the results; choose one and call it `beadpack_denoised`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd008a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!\n",
    "sigma = \n",
    "\n",
    "# The Gaussian...\n",
    "beadpack_gaussian = filters.gaussian()\n",
    "\n",
    "sc.slice_explorer(gaussian);\n",
    "\n",
    "# ... the median...\n",
    "beadpack_rescaled_ubyte = util.img_as_ubyte()\n",
    "beadpack_median = np.empty_like()\n",
    "\n",
    "for plane, image in enumerate(beadpack_rescaled_ubyte):\n",
    "    beadpack_median[plane] = filters.median()\n",
    "    \n",
    "beadpack_median = util.img_as_float(beadpack_median)\n",
    "    \n",
    "sc.slice_explorer(beadpack_median);\n",
    "\n",
    "# ... and the bilateral filters.\n",
    "beadpack_bilateral = np.empty_like()\n",
    "\n",
    "for plane, image in enumerate():\n",
    "    beadpack_bilateral[plane] = restoration.denoise_bilateral(\n",
    "        , \n",
    "        multichannel=False\n",
    "    )\n",
    "\n",
    "sc.slice_explorer(beadpack_bilateral);\n",
    "\n",
    "# Choose your destiny!\n",
    "beadpack_denoised = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b415dd",
   "metadata": {},
   "source": [
    "## Thresholding\n",
    "\n",
    "[Thresholding](https://en.wikipedia.org/wiki/Thresholding_%28image_processing%29) is used to create binary images. A threshold value determines the intensity value separating foreground pixels from background pixels. Foregound pixels are pixels brighter than the threshold value, background pixels are darker. Thresholding is a form of image segmentation.\n",
    "\n",
    "Different thresholding algorithms produce different results. [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method) and Li's minimum cross entropy threshold are two common algorithms. The example below demonstrates how a small difference in the threshold value can visibly alter the binarized image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_li = filters.threshold_li(cells_denoised)\n",
    "cells_binary_li = cells_denoised >= threshold_li\n",
    "\n",
    "threshold_otsu = filters.threshold_otsu(cells_denoised)\n",
    "cells_binary_otsu = cells_denoised >= threshold_otsu\n",
    "\n",
    "_, (win_left, win_center, win_right) = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n",
    "\n",
    "sc.show_plane(win_left, cells_binary_li[32], title='Li\\'s threshold = {:0.2}'.format(threshold_li))\n",
    "sc.show_plane(win_center, cells_binary_otsu[32], title='Otsu\\'s threshold = {:0.2}'.format(threshold_otsu))\n",
    "\n",
    "sc.plot_hist(win_right, cells_denoised, 'Thresholds (Li: red, Otsu: blue)')\n",
    "win_right.axvline(threshold_li, c='r')\n",
    "win_right.axvline(threshold_otsu, c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_binary = cells_binary_li\n",
    "\n",
    "sc.slice_explorer(cells_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279e780",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(5-ish min? 🙄)</font>__ let's binarize `beadpack_denoised`, but using different tools!\n",
    "\n",
    "Your tasks are:\n",
    "  * Use the function `filters.try_all_threshold` to check the binary version of the 100th plane of `beadpack_denoised`.\n",
    "  * Choose one of the thresholds, apply it on the data and call it `beadpack_binary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e214fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!\n",
    "filters.try_all_threshold()\n",
    "\n",
    "threshold = filters.threshold_\n",
    "beadpack_binary = beadpack_denoised >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1ce90",
   "metadata": {},
   "source": [
    "## <a id='morphology'></a>[skimage.morphology](https://scikit-image.org/docs/stable/api/skimage.morphology.html) - binary and grayscale morphology\n",
    "\n",
    "Morphological image processing is a collection of non-linear operations related to the shape or morphology of features in an image, such as boundaries, skeletons, etc. In any given technique, we probe an image with a small shape or template called a structuring element, which defines the region of interest or neighborhood around a pixel.\n",
    "\n",
    "[Mathematical morphology](https://en.wikipedia.org/wiki/Mathematical_morphology) operations and structuring elements are defined in `skimage.morphology`. Structuring elements are shapes which define areas over which an operation is applied. The response to the filter indicates how well the neighborhood corresponds to the structuring element's shape.\n",
    "\n",
    "There are a number of two and three dimensional structuring elements defined in `skimage.morphology`. Not all 2D structuring element have a 3D counterpart. The simplest and most commonly used structuring elements are the `disk`/`ball` and `square`/`cube`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa5c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology  # skimage's morphological submodules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ball = morphology.ball(radius=5)\n",
    "print('* Ball shape: {}'.format(ball.shape))\n",
    "\n",
    "cube = morphology.cube(width=5)\n",
    "print('* Cube shape: {}'.format(cube.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b7c9a",
   "metadata": {},
   "source": [
    "The most basic mathematical morphology operations are `dilation` and `erosion`. Dilation enlarges bright regions and shrinks dark regions. Erosion shrinks bright regions and enlarges dark regions. Other morphological operations are composed of `dilation` and `erosion`.\n",
    "\n",
    "The `closing` of an image is defined as a `dilation` followed by an `erosion`. Closing can remove small dark spots (i.e. “pepper”) and connect small bright cracks. This tends to “close” up (dark) gaps between (bright) features. Morphological `opening` on an image is defined as an `erosion` followed by a `dilation`. Opening can remove small bright spots (i.e. “salt”) and connect small dark cracks. This tends to “open” up (dark) gaps between (bright) features.\n",
    "\n",
    "These operations in `skimage.morphology` are compatible with 3D images and structuring elements. A 2D structuring element cannot be applied to a 3D image, nor can a 3D structuring element be applied to a 2D image.\n",
    "\n",
    "These four operations (`closing`, `dilation`, `erosion`, `opening`) have binary counterparts which are faster to compute than the grayscale algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4900b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = morphology.ball(radius=3)\n",
    "\n",
    "cells_closing = morphology.closing(cells_rescaled, selem=selem)\n",
    "cells_dilation = morphology.dilation(cells_rescaled, selem=selem)\n",
    "cells_erosion = morphology.erosion(cells_rescaled, selem=selem)\n",
    "cells_opening = morphology.opening(cells_rescaled, selem=selem)\n",
    "\n",
    "cells_binary_closing = morphology.binary_closing(cells_binary, selem=selem)\n",
    "cells_binary_dilation = morphology.binary_dilation(cells_binary, selem=selem)\n",
    "cells_binary_erosion = morphology.binary_erosion(cells_binary, selem=selem)\n",
    "cells_binary_opening = morphology.binary_opening(cells_binary, selem=selem)\n",
    "\n",
    "_, ((win_top_1, win_top_2, win_top_3, win_top_4),\n",
    "    (win_bottom_1, win_bottom_2, win_bottom_3, win_bottom_4)) = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n",
    "\n",
    "sc.show_plane(win_top_1, cells_erosion[32], title='Erosion')\n",
    "sc.show_plane(win_top_2, cells_dilation[32], title='Dilation')\n",
    "sc.show_plane(win_top_3, cells_closing[32], title='Closing')\n",
    "sc.show_plane(win_top_4, cells_opening[32], title='Opening')\n",
    "\n",
    "sc.show_plane(win_bottom_1, cells_binary_erosion[32], title='Binary erosion')\n",
    "sc.show_plane(win_bottom_2, cells_binary_dilation[32], title='Binary dilation')\n",
    "sc.show_plane(win_bottom_3, cells_binary_closing[32], title='Binary closing')\n",
    "sc.show_plane(win_bottom_4, cells_binary_opening[32], title='Binary opening')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fed686",
   "metadata": {},
   "source": [
    "Morphology operations can be chained together to denoise an image. For example, a `closing` applied to an `opening` can remove salt and pepper noise from an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c35bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_binary_equalized = cells_equalized >= filters.threshold_li(cells_equalized)\n",
    "\n",
    "cells_despeckled_radius1 = morphology.closing(\n",
    "    morphology.opening(cells_binary_equalized, selem=morphology.ball(1)),\n",
    "    selem=morphology.ball(1)\n",
    ")\n",
    "\n",
    "cells_despeckled_radius3 = morphology.closing(\n",
    "    morphology.opening(cells_binary_equalized, selem=morphology.ball(3)),\n",
    "    selem=morphology.ball(3)\n",
    ")\n",
    "\n",
    "_, (win_left, win_center, win_right) = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\n",
    "\n",
    "sc.show_plane(win_left, cells_binary_equalized[32], title='Noisy data')\n",
    "sc.show_plane(win_center, cells_despeckled_radius1[32], title='Despeckled, r = 1')\n",
    "sc.show_plane(win_right, cells_despeckled_radius3[32], title='Despeckled, r = 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e821e8e3",
   "metadata": {},
   "source": [
    "Functions operating on [connected components](https://en.wikipedia.org/wiki/Connected_space) can remove small undesired elements while preserving larger shapes.\n",
    "\n",
    "`skimage.morphology.remove_small_holes` fills holes and `skimage.morphology.remove_small_objects` removes bright regions. Both functions accept a `min_size` parameter, which is the minimum size (in pixels) of accepted holes or objects. The `min_size` can be approximated by a cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 20\n",
    "\n",
    "cells_remove_holes = morphology.remove_small_holes(\n",
    "    cells_binary,\n",
    "    width ** 3\n",
    ")\n",
    "\n",
    "sc.slice_explorer(cells_remove_holes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 20\n",
    "\n",
    "cells_remove_objects = morphology.remove_small_objects(\n",
    "    cells_remove_holes, \n",
    "    min_size=width ** 3\n",
    ")\n",
    "\n",
    "sc.slice_explorer(cells_remove_objects);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bd879",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(5-ish min? 🙄)</font>__ let's perform some operations on `beadpack_binary` and check the results.\n",
    "\n",
    "Your tasks are:\n",
    "  * Apply opening, closing, dilation and erosion on `beadpack_binary`.\n",
    "  * Generate binary histogram-equalized and CLAHE versions of `beadpack`, according to the threshold you chose previously.\n",
    "  * Remove small holes and objects on `beadpack_binary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d471ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!\n",
    "selem = morphology.ball()\n",
    "\n",
    "beadpack_binary_erosion = morphology.binary_erosion()\n",
    "beadpack_binary_dilation = morphology.binary_dilation()\n",
    "beadpack_binary_closing = morphology.binary_closing()\n",
    "beadpack_binary_opening = morphology.binary_opening()\n",
    "\n",
    "_, (win_1, win_2, win_3, win_4) = plt.subplots(nrows=1, ncols=4, figsize=(16, 5))\n",
    "\n",
    "sc.show_plane(win_bottom_1, , title='Binary erosion')\n",
    "sc.show_plane(win_bottom_2, , title='Binary dilation')\n",
    "sc.show_plane(win_bottom_3, , title='Binary closing')\n",
    "sc.show_plane(win_bottom_4, , title='Binary opening')\n",
    "\n",
    "beadpack_binary_equalized = beadpack_equalized >= filters.threshold_\n",
    "beadpack_binary_clahe = beadpack_clahe >= filters.threshold_\n",
    "\n",
    "width = 20\n",
    "\n",
    "beadpack_remove_holes = morphology.remove_small_holes(\n",
    "    ,\n",
    "    width ** 3\n",
    ")\n",
    "\n",
    "sc.slice_explorer(beadpack_remove_holes);\n",
    "\n",
    "beadpack_remove_objects = morphology.remove_small_objects(\n",
    "    , \n",
    "    min_size=width ** 3\n",
    ")\n",
    "\n",
    "sc.slice_explorer(beadpack_remove_objects);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999912d",
   "metadata": {},
   "source": [
    "## <a id='measure'></a>[skimage.measure](https://scikit-image.org/docs/stable/api/skimage.measure.html) - measuring image or region properties\n",
    "\n",
    "Multiple algorithms to label images, or obtain information about discrete regions of an image.\n",
    "\n",
    "* `measure.label` - Label an image, i.e. identify discrete regions in the image using unique integers.\n",
    "* `measure.regionprops` - In a labeled image, as returned by `label`, find various properties of the labeled regions.\n",
    "\n",
    "Finding paths from a 2D image, or isosurfaces from a 3D image.\n",
    "\n",
    "* `measure.find_contours`\n",
    "* `measure.marching_cubes_lewiner`\n",
    "* `measure.marching_cubes_classic`\n",
    "* `measure.mesh_surface_area` - Surface area of 3D mesh from marching cubes.\n",
    "* `measure.compare_*` - Quantify the difference between two whole images; often used in denoising or restoration.\n",
    "\n",
    "**RANDom Sample Consensus fitting (RANSAC)** - a powerful, robust approach to fitting a model to data.  It exists here because its initial use was for fitting shapes, but it can also fit transforms.\n",
    "* `measure.ransac`\n",
    "* `measure.CircleModel`\n",
    "* `measure.EllipseModel`\n",
    "* `measure.LineModelND`\n",
    "\n",
    "[Image segmentation](https://en.wikipedia.org/wiki/Image_segmentation) partitions images into regions of interest. Integer labels are assigned to each region to distinguish regions of interest.\n",
    "\n",
    "Connected components of the binary image are assigned the same label via `skimage.measure.label`. Tightly packed cells  connected in the binary image are assigned the same label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cef1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure  # skimage's measure submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_labels = measure.label(cells_remove_objects)\n",
    "\n",
    "sc.slice_explorer(cells_labels, cmap='nipy_spectral');\n",
    "\n",
    "_, (win_left, win_center, win_right) = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "sc.show_plane(win_left, cells_rescaled[32, :100, 125:], title='Rescaled')\n",
    "sc.show_plane(win_center, cells_labels[32, :100, 125:], cmap='nipy_spectral', title='Labels')\n",
    "sc.show_plane(win_right, cells_labels[32, :100, 125:] == 8, title='Labels = 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53f021",
   "metadata": {},
   "source": [
    "A better segmentation would assign different labels to disjoint regions in the original image. \n",
    "\n",
    "[Watershed segmentation](https://en.wikipedia.org/wiki/Watershed_%28image_processing%29) can distinguish touching objects. Markers are placed at local minima and expanded outward until there is a collision with markers from another region. The inverse intensity image transforms bright cell regions into basins which should be filled.\n",
    "\n",
    "In declumping, markers are generated from the distance function. Points furthest from an edge have the highest intensity and should be identified as markers using `skimage.feature.peak_local_max`. Regions with pinch points should be assigned multiple markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_distance = ndimage.distance_transform_edt(cells_remove_objects)\n",
    "\n",
    "sc.slice_explorer(cells_distance, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8915f465",
   "metadata": {},
   "source": [
    "## [skimage.feature](https://scikit-image.org/docs/stable/api/skimage.feature.html) - extract features from an image<a id='feature'></a>\n",
    "\n",
    "This submodule presents a diverse set of tools to identify or extract certain features from images, including tools for\n",
    "\n",
    "* Edge detection: `feature.canny`\n",
    "* Corner detection:\n",
    "  * `feature.corner_kitchen_rosenfeld`\n",
    "  * `feature.corner_harris`\n",
    "  * `feature.corner_shi_tomasi`\n",
    "  * `feature.corner_foerstner`\n",
    "  * `feature.subpix`\n",
    "  * `feature.corner_moravec`\n",
    "  * `feature.corner_fast`\n",
    "  * `feature.corner_orientations`\n",
    "* Blob detection\n",
    "  * `feature.blob_dog`\n",
    "  * `feature.blob_doh`\n",
    "  * `feature.blob_log`\n",
    "* Texture\n",
    "  * `feature.greycomatrix`\n",
    "  * `feature.greycoprops`\n",
    "  * `feature.local_binary_pattern`\n",
    "  * `feature.multiblock_lbp`\n",
    "* Peak finding: `feature.peak_local_max`\n",
    "* Object detction\n",
    "  * `feature.hog`\n",
    "  * `feature.match_template`\n",
    "* Stereoscopic depth estimation: `feature.daisy`\n",
    "* Feature matching\n",
    "  * `feature.ORB`\n",
    "  * `feature.BRIEF`\n",
    "  * `feature.CENSURE`\n",
    "  * `feature.match_descriptors`\n",
    "  * `feature.plot_matches`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature  # skimage's feature submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406cf4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_local_max = feature.peak_local_max(\n",
    "    cells_distance,\n",
    "    footprint=np.ones((15, 15, 15), dtype=np.bool),\n",
    "    indices=False,\n",
    "    labels=measure.label(cells_remove_objects)\n",
    ")\n",
    "\n",
    "cells_markers = measure.label(peak_local_max)\n",
    "\n",
    "cells_labels = morphology.watershed(\n",
    "    cells_rescaled, \n",
    "    cells_markers, \n",
    "    mask=cells_remove_objects\n",
    ")\n",
    "\n",
    "sc.slice_explorer(cells_labels, cmap='nipy_spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a956d",
   "metadata": {},
   "source": [
    "After watershed, we have better disambiguation between internal cells.\n",
    "\n",
    "When cells simultaneous touch the border of the image, they may be assigned the same label.  In pre-processing, we typically remove these cells.\n",
    "\n",
    "**Note:** This is 3D data -- you may not always be able to see connections in 2D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4cf2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (win_left, win_right) = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
    "\n",
    "sc.show_plane(win_left, cells_labels[39, 156:, 20:150], cmap='nipy_spectral')\n",
    "sc.show_plane(win_right, cells_labels[34, 90:190, 126:], cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991c904",
   "metadata": {},
   "source": [
    "The watershed algorithm falsely detected subregions in a few cells. This is referred to as oversegmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6592b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axis = plt.subplots()\n",
    "sc.show_plane(axis, cells_labels[38, 50:100, 20:100], cmap='nipy_spectral', title='Oversegmented labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012adb00",
   "metadata": {},
   "source": [
    "Plotting the markers on the distance image reveals the reason for oversegmentation. Cells with multiple markers will be assigned multiple labels, and oversegmented. It can be observed that cells with a uniformly increasing distance map are assigned a single marker near their center. Cells with uneven distance maps are assigned multiple markers, indicating the presence of multiple local maxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 12))\n",
    "\n",
    "vmin = cells_distance.min()\n",
    "vmax = cells_distance.max()\n",
    "\n",
    "offset = 31\n",
    "\n",
    "for index, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(\n",
    "        cells_distance[offset + index],\n",
    "        cmap='gray',\n",
    "        vmin=vmin,\n",
    "        vmax=vmax\n",
    "    )\n",
    "    \n",
    "    peaks = np.nonzero(peak_local_max[offset + index])\n",
    "    \n",
    "    ax.plot(peaks[1], peaks[0], 'r.')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (win_left, win_center, win_right) = plt.subplots(nrows=1, ncols=3, figsize=(16, 8))\n",
    "\n",
    "\n",
    "sc.show_plane(win_left, cells_remove_objects[10:, 193:253, 74])\n",
    "sc.show_plane(win_center, cells_distance[10:, 193:253, 74])\n",
    "\n",
    "features = feature.peak_local_max(cells_distance[10:, 193:253, 74])\n",
    "win_center.plot(features[:, 1], features[:, 0], 'r.')\n",
    "\n",
    "# Improve feature selection by blurring, using a larger footprint\n",
    "# in `peak_local_max`, etc.\n",
    "\n",
    "smooth_distance = filters.gaussian(cells_distance[10:, 193:253, 74], sigma=5)\n",
    "sc.show_plane(win_right, smooth_distance)\n",
    "features = feature.peak_local_max(\n",
    "    smooth_distance\n",
    ")\n",
    "win_right.plot(features[:, 1], features[:, 0], 'bx');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1a471",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(5-ish min? 🙄)</font>__ now it's time to label `beadpack_remove_objects` and separate the beads!\n",
    "\n",
    "Your tasks are:\n",
    "  * Label `beadpack_remove_objects` using `measure.label`, and obtain the distance between the pixels.\n",
    "  * Try different footprints and obtain its max local peaks for `morphology.watershed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "beadpack_labels = measure.label()\n",
    "\n",
    "sc.slice_explorer(beadpack_labels, cmap='nipy_spectral');\n",
    "\n",
    "_, (win_left, win_center, win_right) = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "sc.show_plane(win_left, , title='Rescaled')\n",
    "sc.show_plane(win_center, , cmap='nipy_spectral', title='Labels')\n",
    "sc.show_plane(win_right, , title='Labels = 100')\n",
    "\n",
    "beadpack_distance = ndimage.distance_transform_edt()\n",
    "\n",
    "sc.slice_explorer(, cmap='magma');\n",
    "\n",
    "footprint =\n",
    "\n",
    "peak_local_max = feature.peak_local_max(\n",
    "    ,\n",
    "    footprint=, dtype=np.bool),\n",
    "    indices=False,\n",
    "    labels=measure.label(beadpack_remove_objects)\n",
    ")\n",
    "\n",
    "beadpack_markers = measure.label(peak_local_max)\n",
    "\n",
    "beadpack_labels = morphology.watershed(\n",
    "    beadpack_rescaled, \n",
    "    beadpack_markers, \n",
    "    mask=beadpack_remove_objects\n",
    ")\n",
    "\n",
    "sc.slice_explorer(beadpack_labels, cmap='nipy_spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f9b85",
   "metadata": {},
   "source": [
    "## <a id='segmentation'></a>[skimage.segmentation](https://scikit-image.org/docs/stable/api/skimage.segmentation.html) - identification of regions of interest\n",
    "\n",
    "One of the key image analysis tasks is identifying regions of interest.  These could be a person, an object, certain features of an animal, microscopic image, or stars.  Segmenting an image is the process of determining where these things you want are in your images.\n",
    "\n",
    "Segmentation has two overarching categories:\n",
    "\n",
    "**Supervised** - must provide some guidance (seed points or initial conditions)\n",
    "\n",
    "* `segmentation.random_walker`\n",
    "* `segmentation.active_contour`\n",
    "* `segmentation.watershed`\n",
    "* `segmentation.flood_fill`\n",
    "* `segmentation.flood`\n",
    "\n",
    "**Unsupervised** - no human input\n",
    "\n",
    "* `segmentation.slic`\n",
    "* `segmentation.felzenszwalb`\n",
    "* `segmentation.chan_vese`\n",
    "\n",
    "There are also some supervised and unsupervised thresholding algorithms in `filters`. There is a [segmentation lecture](https://github.com/scikit-image/skimage-tutorials/blob/main/lectures/4_segmentation.ipynb) ([and its solution](https://github.com/scikit-image/skimage-tutorials/blob/main/lectures/solutions/4_segmentation.ipynb)) you may peruse, as well as many [gallery examples](https://scikit-image.org/docs/stable/auto_examples/index.html#segmentation-of-objects) which illustrate all of these segmentation methods.\n",
    "\n",
    "[Feature extraction](https://en.wikipedia.org/wiki/Feature_extraction) reduces data required to describe an image or objects by measuring informative features. These include features such as area or volume, bounding boxes, and intensity statistics.\n",
    "\n",
    "Before measuring objects, it helps to clear objects from the image border. Measurements should only be collected for objects entirely contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43364f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import segmentation  # skimage's segmentation submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae501895",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_labels_inner = segmentation.clear_border(cells_labels)\n",
    "cells_labels_inner = morphology.remove_small_objects(cells_labels_inner, min_size=200)\n",
    "\n",
    "print('Interior labels: {}'.format(np.unique(cells_labels_inner)))\n",
    "\n",
    "sc.slice_explorer(cells_labels_inner, cmap='nipy_spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e179aa",
   "metadata": {},
   "source": [
    "After clearing the border, the object labels are no longer sequentially increasing. The labels can be renumbered such that there are no jumps in the list of image labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_relabeled, _, _ = segmentation.relabel_sequential(cells_labels_inner)\n",
    "\n",
    "print('Relabeled labels: {}'.format(np.unique(cells_relabeled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1ff15",
   "metadata": {},
   "source": [
    "`skimage.measure.regionprops` automatically measures many labeled image features. Optionally, an `intensity_image` can be supplied and intensity features are extracted per object. It's good practice to make measurements on the original image.\n",
    "\n",
    "Not all properties are supported for 3D data. Below are lists of supported and unsupported 3D measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83409586",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = measure.regionprops(cells_relabeled, intensity_image=cells)\n",
    "props_first_region = properties[0]\n",
    "\n",
    "supported = [''] \n",
    "unsupported = ['']\n",
    "\n",
    "for prop in props_first_region:\n",
    "    try:\n",
    "        props_first_region[prop]\n",
    "        supported.append(prop)\n",
    "    except NotImplementedError:\n",
    "        unsupported.append(prop)\n",
    "\n",
    "print('Supported properties:')\n",
    "print('\\n\\t'.join(supported))\n",
    "print()\n",
    "print('Unsupported properties:')\n",
    "print('\\n\\t'.join(unsupported))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63342da3",
   "metadata": {},
   "source": [
    "`skimage.measure.regionprops` ignores the 0 label, which represents the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Measured regions: {}'.format([prop.label for prop in properties]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_volumes = [prop.area for prop in properties]\n",
    "\n",
    "print('Total pixels: {}'.format(cells_volumes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e98cf",
   "metadata": {},
   "source": [
    "Collected measurements can be further reduced by computing per-image statistics such as total, minimum, maximum, mean, and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb676a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Volume statistics\\n')\n",
    "print(' * Total: {}'.format(np.sum(cells_volumes)))\n",
    "print(' * Min: {}'.format(np.min(cells_volumes)))\n",
    "print(' * Max: {}'.format(np.max(cells_volumes)))\n",
    "print(' * Mean: {:0.2f}'.format(np.mean(cells_volumes)))\n",
    "print(' * Standard deviation: {:0.2f}'.format(np.std(cells_volumes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf08216f",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(5-ish min? 🙄)</font>__ let's clean the beads and prepare them to visualization!\n",
    "\n",
    "Here are your tasks:\n",
    "  * Clear the borders and remove small objects on `beadpack_labels`.\n",
    "  * Show the volume information for the beads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c104f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "beadpack_labels_inner = segmentation.clear_border()\n",
    "beadpack_labels_inner = morphology.remove_small_objects()\n",
    "\n",
    "print('Interior labels: {}'.format(np.unique()))\n",
    "\n",
    "sc.slice_explorer(beadpack_labels_inner, cmap='nipy_spectral');\n",
    "\n",
    "beadpack_relabeled, _, _ = segmentation.relabel_sequential(beadpack_labels_inner)\n",
    "\n",
    "print('Relabeled labels: {}'.format(np.unique(beadpack_relabeled)))\n",
    "\n",
    "beadpack_volumes = [prop.area for prop in properties]\n",
    "\n",
    "print('total pixels: {}'.format(beadpack_volumes))\n",
    "\n",
    "print('Volume statistics\\n')\n",
    "print(' * Total: {}'.format(np.sum(beadpack_volumes)))\n",
    "print(' * Min: {}'.format(np.min(beadpack_volumes)))\n",
    "print(' * Max: {}'.format(np.max(beadpack_volumes)))\n",
    "print(' * Mean: {:0.2f}'.format(np.mean(beadpack_volumes)))\n",
    "print(' * Standard deviation: {:0.2f}'.format(np.std(beadpack_volumes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b950d42",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "After cleaning, separating and studying the regions within the data, it's time to visualize them.\n",
    "\n",
    "We can use the perimeters of a region to generate their plots. However, perimeter measurements are not computed for 3D objects. Using the fact that 3D extension of perimeter is surface area, we can measure the surface of an object by generating a surface mesh with `skimage.measure.marching_cubes` and computing the surface area of the mesh with `skimage.measure.mesh_surface_area`. The function `plot_3d_surface` has it covered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.plot_3d_surface(data=cells,\n",
    "                   labels=cells_relabeled,\n",
    "                   region=6,\n",
    "                   spacing=spacing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca3c601",
   "metadata": {},
   "source": [
    "Now let's generate a full, interactive 3D plot using ITK and `itkwidgets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itk\n",
    "from itkwidgets import view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abcb6d0",
   "metadata": {},
   "source": [
    "To generate a 3D plot using ITK, we need to reformat the numpy array into an ITK matrix. Then, we use `itkwidgets.view`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_itk_image = itk.GetImageFromArray(util.img_as_ubyte(cells_relabeled))\n",
    "view(cells_itk_image, ui_collapsed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabb43c",
   "metadata": {},
   "source": [
    "__Exercise: <font color='red'>(3-ish min? 🙄)</font>__ now, using our variable `beadpack_relabeled`, let's check its edges.\n",
    "\n",
    "Your tasks right now are:\n",
    "  * Downscale `beadpack_relabeled` by a factor of 4. \n",
    "  * Convert `beadpack_relabeled` to ITK's image.\n",
    "  * Use ITK's `view` to check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacf75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here!\n",
    "beadpack_relabeled = transform.downscale_local_mean()\n",
    "\n",
    "beadpack_itk_image = itk.GetImageFromArray()\n",
    "view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f279c7e",
   "metadata": {},
   "source": [
    "## ⭐⭐ BONUS! ⭐⭐ Parallelizing image loops\n",
    "\n",
    "In image processing, we frequently apply the same algorithm on a large batch of images. Some of these image loops can take a while to be processed. Here we'll see how to use `joblib` to parallelize loops.\n",
    "\n",
    "Our bilateral application during this tutorial, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44404636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilateral_classic_loop():\n",
    "    cells_bilateral = np.empty_like(cells_rescaled)\n",
    "    for plane, image in enumerate(cells_rescaled):\n",
    "        cells_bilateral[plane] = restoration.denoise_bilateral(image, multichannel=False)\n",
    "    return cells_bilateral\n",
    "\n",
    "%timeit bilateral_classic_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a682e4",
   "metadata": {},
   "source": [
    "Now, let's convert this loop to a `joblib` one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ff27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# when using n_jobs=-2, all CPUs but one are used.\n",
    "\n",
    "def bilateral_joblib_loop():\n",
    "    cells_bilateral = Parallel(n_jobs=-2)(delayed(restoration.denoise_bilateral)(image) for image in cells_rescaled)\n",
    "\n",
    "%timeit bilateral_joblib_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd3b845",
   "metadata": {},
   "source": [
    "## Going beyond\n",
    "\n",
    "[1] A tour/guide on scikit-image's submodules: https://github.com/scikit-image/skimage-tutorials/blob/main/lectures/tour_of_skimage.ipynb\n",
    "\n",
    "[2] scikit-image's gallery examples: https://scikit-image.org/docs/stable/auto_examples/\n",
    "\n",
    "[3] ITK's `ikwidgets`: https://github.com/InsightSoftwareConsortium/itkwidgets\n",
    "\n",
    "[4] `joblib.Parallel`: https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf328cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
